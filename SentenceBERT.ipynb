{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentenceBERT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1XIe9yBpT1Sw9LAh_m-V6GTFUwLJibT5b","authorship_tag":"ABX9TyMf/gaGb7i8cwDOp8fQDkJ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Comment on actual work: \n","\n","The notebook started being based on this tutorial: https://colab.research.google.com/github/rbkhb/NLP_IMC/blob/master/BERT_Fine_Tuning_Sentence_Classification.ipynb#scrollTo=fwQ7JcuJQZ0o\n","\n","But right now the path that seems more logical is https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b \n","\n","Still, after the \"Encode Labels\" part, everything is based on the first reference\n"],"metadata":{"id":"kFbrNX3Hr5ij"}},{"cell_type":"markdown","source":["# **Packages**"],"metadata":{"id":"E9NOFuQgrTBN"}},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bowL6zLU9WPv","executionInfo":{"status":"ok","timestamp":1656509000700,"user_tz":-120,"elapsed":10883,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"a2c21b3f-d061-4cc3-ea24-7428e2410eb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.20.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.24.19)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n","Requirement already satisfied: botocore<1.28.0,>=1.27.19 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.27.19)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.19->boto3->pytorch-pretrained-bert) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.19->boto3->pytorch-pretrained-bert) (1.25.11)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.19->boto3->pytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n"]}],"source":["!pip install -U sentence-transformers\n","!pip install transformers\n","!pip install pytorch-pretrained-bert pytorch-nlp"]},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import transformers\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import csv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","% matplotlib inline\n","from collections import Counter\n","from sklearn.preprocessing import MultiLabelBinarizer"],"metadata":{"id":"v2jk0ONM-U2z","executionInfo":{"status":"ok","timestamp":1656509045974,"user_tz":-120,"elapsed":389,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"czWX7Y7I-Fph","executionInfo":{"status":"ok","timestamp":1656503839416,"user_tz":-120,"elapsed":395,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"2e4938eb-862f-4040-9cd0-d4f29113565b"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# **Dataframe**"],"metadata":{"id":"vgg9sXEkrXDO"}},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/Othercomputers/Mi portátil/Master/GitHub/twsm-group-project/twsm_project_corpus0.csv\")\n","df.head(5)\n","df = df.dropna() #drop na values (somehow a few remained in the dataset)\n","rejoined = [row.split(\" \") for row in df.labels_str]\n","df[\"accept\"] = rejoined\n","df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"EfOhe_Au-eQJ","executionInfo":{"status":"ok","timestamp":1656507214157,"user_tz":-120,"elapsed":388,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"7674d722-ef0b-4f64-ca22-02e9354f4686"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0                                               text  \\\n","0           0  The other part of our data consists of an undi...   \n","1           1  This list of actors can be interpreted to repr...   \n","2           2  The two samples were compared on all study var...   \n","3           3  This is supported by a more recent report by  ...   \n","4           4  , the false noise parameter C was misstated by...   \n","5           5  In the strategic management literature, the na...   \n","6           6  No such link was found however, but, according...   \n","7           7  The problem is that the timeliness of the data...   \n","8           8  The social sciences differ from the physical s...   \n","9           9  Browne et al.  START_CITE [3] END_CITE  CITE_b...   \n","\n","                   accept            labels_str  \n","0                  [Data]                  Data  \n","1            [Definition]            Definition  \n","2        [Method, Theory]         Method Theory  \n","3  [RelationToLiterature]  RelationToLiterature  \n","4            [Definition]            Definition  \n","5  [RelationToLiterature]  RelationToLiterature  \n","6            [FutureWork]            FutureWork  \n","7      [Data, Limitation]       Data Limitation  \n","8  [RelationToLiterature]  RelationToLiterature  \n","9  [RelationToLiterature]  RelationToLiterature  "],"text/html":["\n","  <div id=\"df-28ae9859-3389-4583-b941-c94773ef7808\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>accept</th>\n","      <th>labels_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>The other part of our data consists of an undi...</td>\n","      <td>[Data]</td>\n","      <td>Data</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>This list of actors can be interpreted to repr...</td>\n","      <td>[Definition]</td>\n","      <td>Definition</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>The two samples were compared on all study var...</td>\n","      <td>[Method, Theory]</td>\n","      <td>Method Theory</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>This is supported by a more recent report by  ...</td>\n","      <td>[RelationToLiterature]</td>\n","      <td>RelationToLiterature</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>, the false noise parameter C was misstated by...</td>\n","      <td>[Definition]</td>\n","      <td>Definition</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>In the strategic management literature, the na...</td>\n","      <td>[RelationToLiterature]</td>\n","      <td>RelationToLiterature</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>No such link was found however, but, according...</td>\n","      <td>[FutureWork]</td>\n","      <td>FutureWork</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>The problem is that the timeliness of the data...</td>\n","      <td>[Data, Limitation]</td>\n","      <td>Data Limitation</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>The social sciences differ from the physical s...</td>\n","      <td>[RelationToLiterature]</td>\n","      <td>RelationToLiterature</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Browne et al.  START_CITE [3] END_CITE  CITE_b...</td>\n","      <td>[RelationToLiterature]</td>\n","      <td>RelationToLiterature</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28ae9859-3389-4583-b941-c94773ef7808')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-28ae9859-3389-4583-b941-c94773ef7808 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-28ae9859-3389-4583-b941-c94773ef7808');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["## **Should we apply this pre_process? (Doesnt work yet, tho)**"],"metadata":{"id":"KwEf8F5Nrdo_"}},{"cell_type":"code","source":["def pre_process(text):\n","  text = BeautifulSoup(text).get_text()\n","  # fetch alphabetic characters\n","  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n","  # convert text to lower case\n","  text = text.lower()\n","  # split text into tokens to remove whitespaces\n","  tokens = text.split()\n","  return \" \".join(tokens)"],"metadata":{"id":"R7lxy2lkloUh","executionInfo":{"status":"ok","timestamp":1656508382079,"user_tz":-120,"elapsed":392,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["#fail\n","pre_process(df[\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323},"id":"S-Chumfglxr-","executionInfo":{"status":"error","timestamp":1656508473024,"user_tz":-120,"elapsed":401,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"d9c02927-1030-4d25-ebaa-9cdddb0bfcfb"},"execution_count":53,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-a457499bbd05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-47-d4043ec30e79>\u001b[0m in \u001b[0;36mpre_process\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m# fetch alphabetic characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# convert text to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m              self.builder.prepare_markup(\n\u001b[0;32m--> 279\u001b[0;31m                  markup, from_encoding, exclude_encodings=exclude_encodings)):\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[0;34m(self, markup, user_specified_encoding, exclude_encodings, document_declared_encoding)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtry_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         detector = EncodingDetector(\n\u001b[0;32m--> 121\u001b[0;31m             markup, try_encodings, is_html, exclude_encodings)\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, override_encodings, is_html, exclude_encodings)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# First order of business: strip a byte-order mark.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msniffed_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_byte_order_mark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_usable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mstrip_byte_order_mark\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;31m# Unicode data cannot have a byte-order mark.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'\\xfe\\xff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'\\x00\\x00'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-16be'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1538\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."]}]},{"cell_type":"markdown","source":["## **Max_len decision**"],"metadata":{"id":"BGCiUujXrrrT"}},{"cell_type":"code","source":["#Count words per sentence so we can set an appropiate max_len. In this case, 150 will be more than enough.\n","word_list = [len(x.split()) for x in df[\"text\"].tolist()]\n","fig = plt.figure()\n","ax = fig.add_subplot(1,1,1,)\n","n, bins, patches = ax.hist(word_list, bins=50, histtype='bar')\n","\n","#ax.set_xticklabels([n], rotation='vertical')\n","\n","for patch in patches:\n","    patch.set_facecolor('r')\n","\n","plt.title(\"Words per sentence\")\n","plt.xlabel('Cases')\n","plt.ylabel('Word count')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"5aUMynjJdfJb","executionInfo":{"status":"ok","timestamp":1656506927845,"user_tz":-120,"elapsed":1335,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"b66e36db-51d0-4a45-b660-fe51fdc813b6"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Word count')"]},"metadata":{},"execution_count":34},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Set the maximum sequence length.\n","MAX_LEN = 150"],"metadata":{"id":"tOtL_5narwoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Encode labels**"],"metadata":{"id":"8I5q3_CGrOeg"}},{"cell_type":"code","source":["############# LabelEncoder (1st try)\n","#le = preprocessing.LabelEncoder()\n","#le.fit(df[\"labels_str\"])\n","\n","#le.classes_\n","\n","#df[\"labels_encoded\"]=le.fit_transform(df[\"labels_str\"])\n","\n","#le.inverse_transform([0, 0, 1, 2])"],"metadata":{"id":"U0XlXyMiBb_Y","executionInfo":{"status":"ok","timestamp":1656503993229,"user_tz":-120,"elapsed":2,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#df.head().sort_values(\"labels_encoded\", ascending=True)"],"metadata":{"id":"OmurHLFplK9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Medium tutorial (https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b)\n","\n","mlb = MultiLabelBinarizer()\n","accept_MLB = mlb.fit_transform(df[\"accept\"])\n","# Getting a sense of how the tags data looks like\n","print(accept_MLB[0])\n","print(mlb.inverse_transform(accept_MLB[0].reshape(1,-1)))\n","print(mlb.classes_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDNPjCnThYpn","executionInfo":{"status":"ok","timestamp":1656507924322,"user_tz":-120,"elapsed":405,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"5f9e0fa3-4de1-4301-c6c1-6bce68164cb3"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","[('Data',)]\n","['Contribution' 'Data' 'Definition' 'EmpiricalResult' 'FutureWork' 'Goal'\n"," 'Hypothesis' 'HypothesisResult' 'Item' 'Limitation' 'Method' 'Motivation'\n"," 'RelationToLiterature' 'ResearchQuestion' 'Theory']\n"]}]},{"cell_type":"markdown","source":["# **Tokenize**"],"metadata":{"id":"34Vul4G9sQ3L"}},{"cell_type":"code","source":["#Medium tutorial\n","\n","inputs = self.tokenizer.encode_plus(\n","            df[\"text\"],\n","            None,\n","            add_special_tokens=True,#Add [CLS] [SEP] tokens\n","            max_length= self.max_len,\n","            padding = 'max_length',\n","            return_token_type_ids= False, \n","            return_attention_mask= True,#diff normal/pad tokens\n","            truncation= True,# Truncate data beyond max length\n","            return_tensors = 'pt' # PyTorch Tensor format\n","          )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"uh_rhrptlEsF","executionInfo":{"status":"error","timestamp":1656509341109,"user_tz":-120,"elapsed":910,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"29bb9be8-4f73-4ac5-e35c-a6594654318c"},"execution_count":62,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-fc34ef29fa62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Medium tutorial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m inputs = self.tokenizer.encode_plus(\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"]}]},{"cell_type":"markdown","source":["## **From here on, everything is from the first reference https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=e5vZhQpvkE8s**"],"metadata":{"id":"TncdPVdgsbXW"}},{"cell_type":"code","source":["# Create sentence and label lists\n","#sentences = df.text.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n","#sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n","#labels = df.accept_MLB"],"metadata":{"id":"E13E0scl_HnS","executionInfo":{"status":"ok","timestamp":1656504567402,"user_tz":-120,"elapsed":396,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["num_labels = df.labels_encoded.nunique()\n","print(num_labels)\n","#df.labels_encoded.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbse_JnGMnoB","executionInfo":{"status":"ok","timestamp":1656504169268,"user_tz":-120,"elapsed":949,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"6f8ab72c-7029-4473-afed-f3ebe275c6fb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["153\n"]}]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","print (\"First sentence tokenized\")\n","print (tokenized_texts[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DDWnsLc9_YkE","executionInfo":{"status":"ok","timestamp":1656350470399,"user_tz":-120,"elapsed":1055,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"d9226f48-d350-44ff-b199-5abfa42bf5a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 231508/231508 [00:00<00:00, 2844223.12B/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenize the first sentence:\n","['[CLS]', 'the', 'other', 'part', 'of', 'our', 'data', 'consists', 'of', 'an', 'und', '##ire', '##cted', 'social', 'graph', '.', '[SEP]']\n"]}]},{"cell_type":"code","source":["# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"],"metadata":{"id":"-fNbnhIxAKan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"metadata":{"id":"SgcBgX6CAV95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)"],"metadata":{"id":"O9aioxYYAZzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use train_test_split to split our data into train and validation sets for training\n","\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.05)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.05)"],"metadata":{"id":"Ms3nXvBbAdDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert all of our data into torch tensors, the required datatype for our model\n","\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"o6BSYQyZAjDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n","batch_size = 8\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"metadata":{"id":"xf7TemZrKoqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=num_labels, id2label=id2label, label2id=label2id) #number of unique values in labels, probably should be reduced\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414},"id":"gmCmW3jaKrLO","executionInfo":{"status":"error","timestamp":1656504185717,"user_tz":-120,"elapsed":5876,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"7e8b285c-a5ff-4a88-d060-9bc23dc817ef"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["  6%|▌         | 23532544/407873900 [00:03<00:35, 10857044.23B/s]"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-60fff85bf60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#number of unique values in labels, probably should be reduced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             logger.error(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# we are copying the file before closing it, so flush to avoid truncation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;31m# check counter first to reduce calls to time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_n\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminiters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0mcur_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]"],"metadata":{"id":"I6MWVGKTKw0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters for training loop\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                     lr=2e-5,\n","                     warmup=.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0c1eZDXK2xF","executionInfo":{"status":"ok","timestamp":1656350818281,"user_tz":-120,"elapsed":192,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"ec90cd65-7b3d-4090-e791-fb9b2520c5a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["t_total value of -1 results in schedule not being applied\n"]}]},{"cell_type":"code","source":["# Function to calculate the accuracy of predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"qHM_jw7wK9by"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","torch.cuda.memory_summary(device=None, abbreviated=False)"],"metadata":{"id":"XexSTpSZNrUa","executionInfo":{"status":"ok","timestamp":1656350819949,"user_tz":-120,"elapsed":192,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"7543ae0d-86b2-4c03-aa22-0678f93c08c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 3            |        cudaMalloc retries: 3         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   13618 MB |   13620 MB |   26772 MB |   13154 MB |\\n|       from large pool |   13616 MB |   13618 MB |   26768 MB |   13152 MB |\\n|       from small pool |       2 MB |       2 MB |       4 MB |       2 MB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   13618 MB |   13620 MB |   26772 MB |   13154 MB |\\n|       from large pool |   13616 MB |   13618 MB |   26768 MB |   13152 MB |\\n|       from small pool |       2 MB |       2 MB |       4 MB |       2 MB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   13672 MB |   13674 MB |   13744 MB |   73728 KB |\\n|       from large pool |   13668 MB |   13670 MB |   13740 MB |   73728 KB |\\n|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   54808 KB |  225737 KB |    4775 MB |    4721 MB |\\n|       from large pool |   52992 KB |  225024 KB |    4769 MB |    4717 MB |\\n|       from small pool |    1816 KB |    2121 KB |       6 MB |       4 MB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     496    |     497    |     804    |     308    |\\n|       from large pool |     328    |     328    |     558    |     230    |\\n|       from small pool |     168    |     169    |     246    |      78    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     496    |     497    |     804    |     308    |\\n|       from large pool |     328    |     328    |     558    |     230    |\\n|       from small pool |     168    |     169    |     246    |      78    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     176    |     176    |     179    |       3    |\\n|       from large pool |     174    |     174    |     177    |       3    |\\n|       from small pool |       2    |       2    |       2    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      22    |      27    |     133    |     111    |\\n|       from large pool |      19    |      22    |      80    |      61    |\\n|       from small pool |       3    |       6    |      53    |      50    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Store our loss and accuracy for plotting\n","train_loss_set = []\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  \n","  \n","  # Training\n","  \n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","    # Forward pass\n","    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","    train_loss_set.append(loss.item())    \n","    # Backward pass\n","    loss.backward()\n","    # Update parameters and take a step using the computed gradient\n","    optimizer.step()\n","    \n","    \n","    # Update tracking variables\n","    tr_loss += loss.item()\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","    \n","    \n","  # Validation\n","\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"SeM-EQ2WLCFC","executionInfo":{"status":"error","timestamp":1656350821262,"user_tz":-120,"elapsed":219,"user":{"displayName":"Roger Pujol","userId":"07983155497047424279"}},"outputId":"3294ff1d-2a40-4c8d-e59a-996bf01380dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-0e14ac0ae772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtrain_loss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 14.76 GiB total capacity; 13.32 GiB already allocated; 9.75 MiB free; 13.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_loss_set)\n","plt.show()"],"metadata":{"id":"azDDIbI5X60X"},"execution_count":null,"outputs":[]}]}